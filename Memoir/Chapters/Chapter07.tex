%************************************************
\chapter{Evaluación, Comparación y Discusión de Resultados}
\label{ch:eval}
%************************************************

El conjunto de datos usado ha sido \emph{Spanish Universal Dependency}, para
poder ejecutar el algoritmo con estos datos, ha sido necesario hacer una
conversión de los mismos, ya que el \emph{\ac{POS} tagger} de Stanford no
etiqueta los datos de acuerdo al
estandar\footnote{\url{http://universaldependencies.github.io/docs/u/pos
    /index.html}}. El \emph{tagger} de Stanford utiliza las etiquetas de
\emph{AnCora 3.0}\footnote{\url{http://clic.ub.edu/corpus/en}}. Para dicha
conversión se usó un \emph{script} en \textsc{python} proporcionado por
\citeauthor{rohit2016} \cite{rohit2016}.

\section{Medidas de Evaluación}
\label{sec:eval}

En orden de evaluar los resultados del algoritmo, \citeauthor{yamada2003}
proponen tres tipos de medidas. Precisión de Dependencias, Precisión en la Raíz
y clasificación Completa --- \emph{Dependency Accuracy (Dep. Acc.), Root
  Accuracy (Root Acc.)} y \emph{Complete Rate (Comp. Rate)},
respectivamente. --- Dichas mediciones se definen como
\begin{equation*}
  \begin{aligned}
    & \text{\emph{Dep. Acc}} &=& \quad\frac{\text{Número correcto de padres}}{\text{Número
        total de padres}} \\
    & \text{\emph{Root Acc}} &=& \quad\frac{\text{Número de nodos raíz correctos}}{\text{Número
        total de frases}} \\
    & \text{\emph{Comp. Rate}} &=& \quad\frac{\text{Número de frases
        parseadas por completo}}{\text{Número total de frases}} 
  \end{aligned}
\end{equation*}
Para las pruebas realizadas, el tamaño del conjunto de datos de entrenamiento es
de 14305 frases, los datos de \emph{test} contienen 1721 sentencias.

\section{Comparación de Resultados}
\label{sec:results}

Tras realizar varias pruebas para ajustar parámetros, finalmente se fijó el
grado del polinómio a 2, como muestra el Código~\ref{lst:svmparams}. En cuanto a
la longitud del contexto, introducido en \autoref{subsec:featureextraction} el
mejor resultado se obtiene cuando se fija en $(2,4)$, esto es, se usa un
contexto a la izquierda de dos nodos, y un contexto a la derecha de cuatro
nodos. La \autoref{tab:results} muestra una comparación de los resultados
obtenidos con el parseador implementado frente a los resultados de
\citeauthor{rohit2016}.
\begin{table}[ht]
  \myfloatalign
  \begin{tabular}{l|cc}
    \tableheadline{Kernel: $(x'\cdot x'' + 1)^2$, Contexto: $(2,4)$ }
       & \tableheadline{TFG}
       & \tableheadline{\citeauthor{rohit2016}} \\
    \toprule
    \emph{Dep. Acc.}  & 76\%   & 75\% \\
    \emph{Root Acc.}  & 67\%   & 70\% \\
    \emph{Comp. Rate} & 15\%   & 11\% \\
    \bottomrule
  \end{tabular}
  \caption{Comparación de resultados}
  \label{tab:results}
\end{table}

Como se puede apreciar, los resultados son bastante similares, llegando a
mejorar el \emph{Comp. Rate}.


\myTodo[inline]{
Tienes que hablar del llamado experimental framework, o marco experimental, que incluye:}
\myTodo[inline]{- Datos o bases de datos a tratar. Proporciona toda la información posible sobre ello, no solo su tamaño. De qué se trata, dónde están, etc etc.}
\myTodo[inline]{- Medidas de evaluación (no métodos).}
\myTodo[inline]{- De qué va el método de Jain? Hay que describirlo también con el mayor posible nivel de detalle.}
\myTodo[inline]{- Breve análisis de resultados, ¿por qué sale lo que sale o por qué crees que sale eso?}


%*****************************************
%*****************************************
%*****************************************
%*****************************************
%*****************************************
